{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "certain-explosion",
   "metadata": {},
   "source": [
    "# Train a TensorFlow 2.x model with the Amazon SageMaker optimized TensorFlow container and debug using Amazon SageMaker Debugger\n",
    "\n",
    "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed machine learning service. With SageMaker, you have the option of using the built-in algorithms as well as bringing your own algorithms and frameworks.  One such framework is TensorFlow 2.x.  [Amazon SageMaker Debugger](https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html) debugs, monitors and profiles training jobs in real time thereby helping with detecting non-converging conditions, optimizing resource utilization by eliminating bottlenecks, improving training time and reducing costs of your machine learning models.\n",
    "\n",
    "This notebook demonstrates how to use a SageMaker optimized TensorFlow 2.x container to train a multi-class image classification model using the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) and debug using SageMaker Debugger.  Finally the debugger's output is analyzed.  This will take your training script and use SageMaker in script mode.\n",
    "\n",
    "This notebook will create resources in the same AWS account and in the same region where this notebook is running.\n",
    "\n",
    "1. [Complete prerequisites](#Complete%20prerequisites)\n",
    "\n",
    "    1. [Check and configure access to the Internet](#Check%20and%20configure%20access%20to%20the%20Internet)\n",
    "\n",
    "    2. [Check and upgrade required software versions](#Check%20and%20upgrade%20required%20software%20versions)\n",
    "    \n",
    "    3. [Check and configure security permissions](#Check%20and%20configure%20security%20permissions)\n",
    "\n",
    "    4. [Organize imports](#Organize%20imports)\n",
    "    \n",
    "    5. [Create common objects](#Create%20common%20objects)\n",
    "\n",
    "2. [Prepare the dataset](#Prepare%20the%20dataset)\n",
    "\n",
    "    1. [Create the local directories](#Create%20the%20local%20directories)\n",
    "\n",
    "    2. [Load the dataset](#Load%20the%20dataset)\n",
    "    \n",
    "    3. [View the details of the dataset](#View%20the%20details%20of%20the%20dataset)\n",
    "    \n",
    "    4. [Visualize the dataset](#Visualize%20the%20dataset)\n",
    "    \n",
    "    5. [Normalize the dataset](#Normalize%20the%20dataset)\n",
    "    \n",
    "    6. [Save the prepared datasets locally](#Save%20the%20prepared%20datasets%20locally)\n",
    "    \n",
    "    7. [Upload the prepared datasets to S3](#Upload%20the%20prepared%20datasets%20to%20S3)\n",
    "\n",
    "3. [View the training script](#View%20the%20training%20script)\n",
    "\n",
    "4. [Perform training, validation and testing](#Perform%20training%20validation%20and%20testing)\n",
    "\n",
    "    1. [Set the training parameters](#Set%20the%20training%20parameters)\n",
    "    \n",
    "    2. [Set the debugger parameters](#Set%20the%20debugger%20parameters)\n",
    "\n",
    "    3. [Run the training job](#Run%20the%20training%20job)\n",
    "\n",
    "5. [View the auto-generated debugger profiling report](#View%20the%20auto-generated%20debugger%20profiling%20report)\n",
    "\n",
    "6. [Perform custom analysis of the debugger output](#Perform%20custom%20analysis%20of%20the%20debugger%20output)\n",
    "\n",
    "    1. [Get the training job](#Get%20the%20training%20job)\n",
    "\n",
    "    2. [Read the metrics](#Read%20the%20metrics)\n",
    "\n",
    "    3. [Plot the metrics](#Plot%20the%20metrics)\n",
    "    \n",
    "        1. [System metrics histogram](#System%20metrics%20histogram)\n",
    "\n",
    "        2. [Framework metrics stepline chart](#Framework%20metrics%20stepline%20chart)\n",
    "        \n",
    "        3. [Framework metrics step histogram](#Framework%20metrics%20step%20histogram)\n",
    "\n",
    "        4. [System and framework metrics timeline charts](#System%20and%20framework%20metrics%20timeline%20charts)\n",
    "\n",
    "        5. [System and framework metrics heatmap](#System%20and%20framework%20metrics%20heatmap)\n",
    "\n",
    "7. [Cleanup](#Cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-birthday",
   "metadata": {},
   "source": [
    "##  1. Complete prerequisites <a id ='Complete%20prerequisites'> </a>\n",
    "\n",
    "Check and complete the prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-jones",
   "metadata": {},
   "source": [
    "###  A. Check and configure access to the Internet <a id ='Check%20and%20configure%20access%20to%20the%20Internet'> </a>\n",
    "This notebook requires outbound access to the Internet to download the required software updates and to download the dataset.  You can either provide direct Internet access (default) or provide Internet access through a VPC.  For more information on this, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-latin",
   "metadata": {},
   "source": [
    "###  B. Check and upgrade required software versions <a id ='Check%20and%20upgrade%20required%20software%20versions'> </a>\n",
    "\n",
    "This notebook requires:\n",
    "* [SageMaker Python SDK version 2.x](https://sagemaker.readthedocs.io/en/stable/v2.html)\n",
    "* [TensorFlow version 2.x with SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html)\n",
    "* [Python 3.6.x](https://www.python.org/downloads/release/python-360/)\n",
    "* [SMDebug](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-analyze-data.html)\n",
    "* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "\n",
    "Note: If you get 'module not found' errors in the following cell, then uncomment the appropriate installation commands and install the modules.  Also, uncomment and run the kernel shutdown command.  When the kernel comes back, comment out the installation and kernel shutdown commands and run the following cell.  Now, you should not see any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-copper",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import IPython\n",
    "import sagemaker\n",
    "import smdebug\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# Install/upgrade sagemaker, boto3 smdebug and tensorflow\n",
    "#!{sys.executable} -m pip install -U sagemaker boto3 smdebug\n",
    "#!{sys.executable} -m pip install -U tensorflow\n",
    "#IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "# Get the current installed version of Sagemaker SDK, TensorFlow, Python and Boto3\n",
    "print('SageMaker Python SDK version : {}'.format(sagemaker.__version__))\n",
    "print('TensorFlow version : {}'.format(tf.__version__))\n",
    "print('Python version : {}'.format(sys.version))\n",
    "print('Boto3 version : {}'.format(boto3.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-blind",
   "metadata": {},
   "source": [
    "###  C. Check and configure security permissions <a id ='Check%20and%20configure%20security%20permissions'> </a>\n",
    "This notebook uses the IAM role attached to the underlying notebook instance.  To view the name of this role, run the following cell.\n",
    "\n",
    "Note: This role should have the following permissions,\n",
    "\n",
    "1. Full access to the S3 bucket that will be used to store training and output data.\n",
    "2. Full access to launch training instances.\n",
    "3. Access to write to CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sagemaker.get_execution_role())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-instruction",
   "metadata": {},
   "source": [
    "###  D. Organize imports <a id ='Organize%20imports'> </a>\n",
    "\n",
    "Organize all the library and module imports for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-gnome",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sagemaker.debugger import (ProfilerConfig,\n",
    "                                FrameworkProfile,\n",
    "                                CollectionConfig,\n",
    "                                DebuggerHookConfig,\n",
    "                                DetailedProfilingConfig, \n",
    "                                DataloaderProfilingConfig, \n",
    "                                PythonProfilingConfig,\n",
    "                                Rule,\n",
    "                                PythonProfiler,\n",
    "                                cProfileTimer,\n",
    "                                ProfilerRule,\n",
    "                                rule_configs)\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from smdebug.profiler.analysis.notebook_utils.metrics_histogram import MetricsHistogram\n",
    "from smdebug.profiler.analysis.notebook_utils.step_timeline_chart import StepTimelineChart\n",
    "from smdebug.profiler.analysis.notebook_utils.step_histogram import StepHistogram\n",
    "from smdebug.profiler.analysis.notebook_utils.timeline_charts import TimelineCharts\n",
    "from smdebug.profiler.analysis.notebook_utils.heatmap import Heatmap\n",
    "from smdebug.profiler.analysis.notebook_utils.training_job import TrainingJob\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-bradford",
   "metadata": {},
   "source": [
    "###  E. Create common objects <a id='Create%20common%20objects'></a>\n",
    "\n",
    "Create common objects to be used in future steps in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the S3 bucket name\n",
    "s3_bucket = '<Specify the S3 bucket name>'\n",
    "\n",
    "# Create the S3 Boto3 resource\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_bucket_resource = s3_resource.Bucket(s3_bucket)\n",
    "\n",
    "# Base name to be used to create resources\n",
    "nb_name = 'tf2-fashion-mnist-debugger'\n",
    "\n",
    "# Names of various resources\n",
    "train_job_name = 'train-{}'.format(nb_name)\n",
    "\n",
    "# Names of local sub-directories in the notebook file system\n",
    "data_dir = os.path.join(os.getcwd(), 'data/{}'.format(nb_name))\n",
    "train_dir = os.path.join(os.getcwd(), 'data/{}/train'.format(nb_name))\n",
    "test_dir = os.path.join(os.getcwd(), 'data/{}/test'.format(nb_name))\n",
    "\n",
    "# Sub-folder names in S3\n",
    "train_dir_s3_prefix = '{}/data/train'.format(nb_name)\n",
    "test_dir_s3_prefix = '{}/data/test'.format(nb_name)\n",
    "\n",
    "# Location in S3 where the training scripts will be copied\n",
    "code_location = 's3://{}/{}/scripts'.format(s3_bucket, nb_name)\n",
    "\n",
    "# Location in S3 where the model checkpoint will be stored\n",
    "model_checkpoint_s3_path = 's3://{}/{}/checkpoint/'.format(s3_bucket, nb_name)\n",
    "\n",
    "# Location in S3 where the trained model and debugger output will be stored\n",
    "model_and_debugger_output_s3_path = 's3://{}/{}/output/'.format(s3_bucket, nb_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-nursing",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset <a id ='Prepare%20the%20dataset'> </a>\n",
    "\n",
    "The [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist) consists of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images.  These categories are mapped to integers from 0 to 9 and represent the following class labels,\n",
    "\n",
    "* 0: T-shirt/top\n",
    "* 1: Trouser\n",
    "* 2: Pullover\n",
    "* 3: Dress\n",
    "* 4: Coat\n",
    "* 5: Sandal\n",
    "* 6: Shirt\n",
    "* 7: Sneaker\n",
    "* 8: Bag\n",
    "* 9: Ankle boot\n",
    "\n",
    "The following steps will help with preparing the dataset for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-promise",
   "metadata": {},
   "source": [
    "### A) Create the local directories <a id='Create%20the%20local%20directories'></a>\n",
    "\n",
    "Create the directories in the local system where the dataset will be copied to and processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the local directories\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-christmas",
   "metadata": {},
   "source": [
    "### B) Load the dataset <a id ='Load%20the%20dataset'> </a>\n",
    "\n",
    "Load the pre-shuffled train and test data with the keras.datasets API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-provider",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-ebony",
   "metadata": {},
   "source": [
    "### C) View the details of the dataset <a id ='View%20the%20details%20of%20the%20dataset'> </a>\n",
    "\n",
    "Print the shape of the data and you will notice that they are 28x28 pixels.  There are 60,000 images in the training data and 10,000 images in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-appraisal",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Summarize the dataset\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-celebrity",
   "metadata": {},
   "source": [
    "### D) Visualize the dataset <a id ='Visualize%20the%20dataset'> </a>\n",
    "\n",
    "Randomly display the images and labels of `sample_size` number of images from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-above",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Randomly display the images and labels of n (sample_size) images from the test dataset\n",
    "\n",
    "sample_size = 50\n",
    "\n",
    "random_indexes = np.random.randint(0, len(x_test), sample_size)\n",
    "sample_images = x_test[random_indexes]\n",
    "sample_labels = y_test[random_indexes]\n",
    "sample_predictions = None\n",
    "num_rows = 5\n",
    "num_cols = 10\n",
    "plot_title = None\n",
    "fig_size = None\n",
    "assert sample_images.shape[0] == num_rows * num_cols\n",
    "\n",
    "# Labels\n",
    "FASHION_LABELS = {\n",
    "    0: 'T-shirt/top',\n",
    "    1: 'Trouser',\n",
    "    2: 'Pullover',\n",
    "    3: 'Dress',\n",
    "    4: 'Coat',\n",
    "    5: 'Sandal',\n",
    "    6: 'Shirt',\n",
    "    7: 'Sneaker',\n",
    "    8: 'Bag',\n",
    "    9: 'Ankle boot'\n",
    "}\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    sns.set_context(\"notebook\", font_scale=1.1)\n",
    "    sns.set_style({\"font.sans-serif\": [\"Verdana\", \"Arial\", \"Calibri\", \"DejaVu Sans\"]})\n",
    "    f, ax = plt.subplots(num_rows, num_cols, figsize=((14, 9) if fig_size is None else fig_size),\n",
    "        gridspec_kw={\"wspace\": 0.02, \"hspace\": 0.30}, squeeze=True)\n",
    "    for r in range(num_rows):\n",
    "        for c in range(num_cols):\n",
    "            image_index = r * num_cols + c\n",
    "            ax[r, c].axis(\"off\")\n",
    "            ax[r, c].imshow(sample_images[image_index], cmap=\"Greys\")\n",
    "            if sample_predictions is None:\n",
    "                title = ax[r, c].set_title(\"%s\" % FASHION_LABELS[sample_labels[image_index]])\n",
    "            else:\n",
    "                true_label = sample_labels[image_index]\n",
    "                pred_label = sample_predictions[image_index]\n",
    "                prediction_matches_true = (sample_labels[image_index] == sample_predictions[image_index])\n",
    "                if prediction_matches_true:\n",
    "                    title = FASHION_LABELS[true_label]\n",
    "                    title_color = 'g'\n",
    "                else:\n",
    "                    title = '%s/%s' % (FASHION_LABELS[true_label], FASHION_LABELS[pred_label])\n",
    "                    title_color = 'r'\n",
    "                title = ax[r, c].set_title(title)\n",
    "                plt.setp(title, color=title_color)\n",
    "    if plot_title is not None:\n",
    "        f.suptitle(plot_title)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-right",
   "metadata": {},
   "source": [
    "The pixel values of the images fall in the range of 0 to 255. You can verify this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-picking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(x_train[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-isolation",
   "metadata": {},
   "source": [
    "### E) Normalize the dataset <a id ='Normalize%20the%20dataset'> </a>\n",
    "\n",
    "As the pixel values range from 0 to 255, it is important to normalize them to a range from 0 to 1.  This can be done by dividing these values by 255.  This has to be done for both training and test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-blocking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-recommendation",
   "metadata": {},
   "source": [
    "### F) Save the prepared datasets locally <a id='Save%20the%20prepared%20datasets%20locally'></a>\n",
    "\n",
    "Save the prepared train, validate and test datasets to local directories.  Prior to saving, concatenate x and y columns as needed.  Create the directories if they don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-applicant",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save the prepared dataset (in numpy format) to the local directories\n",
    "np.save(os.path.join(train_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-roommate",
   "metadata": {},
   "source": [
    "### G) Upload the prepared datasets to S3 <a id ='Upload%20the%20prepared%20datasets%20to%20S3'> </a>\n",
    "\n",
    "Upload the datasets from the local directories to appropriate sub-directories in the specified S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-village",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Upload the data to S3\n",
    "train_data_s3_full_path = sagemaker.Session().upload_data(path='./data/{}/train/'.format(nb_name),\n",
    "                                                          bucket=s3_bucket,\n",
    "                                                          key_prefix=train_dir_s3_prefix)\n",
    "test_data_s3_full_path = sagemaker.Session().upload_data(path='./data/{}/test/'.format(nb_name),\n",
    "                                                         bucket=s3_bucket,\n",
    "                                                         key_prefix=test_dir_s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-feedback",
   "metadata": {},
   "source": [
    "## 3. View the training script <a id ='View%20the%20training%20script'> </a>\n",
    "\n",
    "View the script that will be used for training the model.  This should exist in a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-arrival",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat scripts/train_tf2_fashion_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-telescope",
   "metadata": {},
   "source": [
    "##  4. Perform training, validation and testing <a id ='Perform%20training%20validation%20and%20testing'> </a>\n",
    "\n",
    "In this step, we will use a SageMaker optimized TensorFlow 2.x container to train a multi-class image classification model using the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist).  This will take your training script and use SageMaker in script mode.\n",
    "\n",
    "Debugger will be enabled as part of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-transportation",
   "metadata": {},
   "source": [
    "### A) Set the training parameters <a id ='Set%20the%20training%20parameters'> </a>\n",
    "\n",
    "1. Inputs - S3 locations for training and test data.\n",
    "2. Hyperparameters.\n",
    "3. Training instance details:\n",
    "\n",
    "    1. Instance count\n",
    "    \n",
    "    2. Instance type\n",
    "    \n",
    "    3. The max run time of the training job\n",
    "    \n",
    "    4. (Optional) Use Spot instances.  For more info, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html).\n",
    "    \n",
    "    5. (Optional) The max wait for Spot instances, if using Spot.  This should be larger than the max run time.\n",
    "    \n",
    "4. Tensorflow framework version and Python version.\n",
    "5. Names of the training script and the local directory where it is located.\n",
    "6. Logging level of the SageMaker optimized Tensorflow 2.x container.\n",
    "7. Appropriate local and S3 directories that will be used by the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-sheep",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set the input data paths\n",
    "inputs = {'train':train_data_s3_full_path, 'test':test_data_s3_full_path}\n",
    "\n",
    "# Set the hyperparameters\n",
    "hyperparameters = {'epochs': 25,\n",
    "                   'batch_size': 32,\n",
    "                   'learning_rate': 0.001,\n",
    "                   'decay': 1e-6}\n",
    "\n",
    "# Set the instance count, instance type, options to use Spot instances, job name and other parameters\n",
    "train_instance_count = 1\n",
    "train_instance_type = 'ml.p3.16xlarge'\n",
    "#use_spot_instances = True\n",
    "#spot_max_wait_time_in_seconds = 5400\n",
    "use_spot_instances = False\n",
    "spot_max_wait_time_in_seconds = None\n",
    "max_run_time_in_seconds = 3600\n",
    "\n",
    "framework_version = '2.4.1'\n",
    "py_version = 'py37'\n",
    "\n",
    "# Set the training script related parameters\n",
    "train_script_dir = 'scripts'\n",
    "train_script = 'train_tf2_fashion_mnist.py'\n",
    "\n",
    "# Set the training container related parameters\n",
    "container_log_level = logging.INFO\n",
    "\n",
    "# Location where the model checkpoints will be stored locally in the container before being uploaded to S3\n",
    "model_checkpoint_local_dir = '/opt/ml/checkpoints/'\n",
    "\n",
    "# Location where the trained model will be stored locally in the container before being uploaded to S3\n",
    "model_local_dir = '/opt/ml/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anonymous-agenda",
   "metadata": {},
   "source": [
    "### B) Set the debugger parameters <a id ='Set%20the%20debugger%20parameters'> </a>\n",
    "\n",
    "1. **Profile config** - configure how to collect system metrics and framework metrics from your training job and save into your secured S3 bucket URI or local machine.\n",
    "\n",
    "    1. [Monitoring hardware system resource utilization](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-configure-system-monitoring.html)\n",
    "  \n",
    "    2. [Framework profiling](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-configure-framework-profiling.html)\n",
    "  \n",
    "2. **Debugger hook config** - configure how to collect output tensors from your training job and save into your secured S3 bucket URI or local machine.  For more info, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-configure-hook.html).\n",
    "\n",
    "3. **Rules** - configure this parameter to enable Debugger built-in rules that you want to run in parallel. The rules automatically analyze your training job and find training issues. The ProfilerReport rule saves the Debugger profiling reports in your secured S3 bucket URI.  For more info, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/use-debugger-built-in-rules.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-nancy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Location in S3 where the debugger output will be stored is mentioned in the previous step\n",
    "\n",
    "# Set the profile config for both system and framework metrics\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis = 1000,\n",
    "    framework_profile_params = FrameworkProfile(\n",
    "        detailed_profiling_config = DetailedProfilingConfig(\n",
    "            start_step = 5, \n",
    "            num_steps = 10\n",
    "        ),\n",
    "        dataloader_profiling_config = DataloaderProfilingConfig(\n",
    "            start_step = 7, \n",
    "            num_steps = 10\n",
    "        ),\n",
    "        python_profiling_config = PythonProfilingConfig(\n",
    "            start_step = 9, \n",
    "            num_steps = 10,\n",
    "            python_profiler = PythonProfiler.CPROFILE, \n",
    "            cprofile_timer = cProfileTimer.TOTAL_TIME\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set the debugger hook config to save tensors\n",
    "debugger_hook_config = DebuggerHookConfig(\n",
    "    collection_configs = [\n",
    "        CollectionConfig(name = \"weights\"),\n",
    "        CollectionConfig(name = \"gradients\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set the rules to analyze tensors emitted during training\n",
    "rules = [\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.stalled_training_rule())\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-catch",
   "metadata": {},
   "source": [
    "### C) Run the training job <a id='Run%20the%20training%20job'></a>\n",
    "\n",
    "Prepare the `estimator` and call the `fit()` method.  This will pull the container containing the specified version of the framework in the AWS region, copy the training script to it and run the training job in the specified type of EC2 instance(s).  The training data will be pulled from the specified location in S3 and the generated model along with the checkpoints will be written to the specified locations in S3.  The debugger will use its configured settings to capture the data and write them to the specified locations in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-screen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the estimator\n",
    "estimator = TensorFlow(\n",
    "    source_dir=train_script_dir,\n",
    "    entry_point=train_script,\n",
    "    code_location=code_location,\n",
    "    checkpoint_local_path=model_checkpoint_local_dir,\n",
    "    checkpoint_s3_uri=model_checkpoint_s3_path,\n",
    "    model_dir=model_local_dir,\n",
    "    output_path=model_and_debugger_output_s3_path,\n",
    "    instance_type=train_instance_type,\n",
    "    instance_count=train_instance_count,\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_wait=spot_max_wait_time_in_seconds,\n",
    "    max_run=max_run_time_in_seconds,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    base_job_name=train_job_name,\n",
    "    framework_version=framework_version,\n",
    "    py_version=py_version,\n",
    "    container_log_level=container_log_level,\n",
    "    script_mode=True,\n",
    "    disable_profiler=False,\n",
    "    profiler_config=profiler_config,\n",
    "    debugger_hook_config=debugger_hook_config,\n",
    "    rules=rules)\n",
    "\n",
    "# Perform the training\n",
    "estimator.fit(inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-commissioner",
   "metadata": {},
   "source": [
    "## 5. View the auto-generated debugger profiling report <a id ='View%20the%20auto-generated%20debugger%20profiling%20report'> </a>\n",
    "\n",
    "The debugger's auto-generated profiling report will be stored in the S3 directory specified in earlier steps.  You can view it here.\n",
    "\n",
    "For information on how to read the report, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-profiling-report.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-porter",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the S3 path to the debugger's auto-generated profiling report\n",
    "profiling_report_s3_prefix = '{}/output/{}/rule-output/ProfilerReport/profiler-output/profiler-report.html'.format(nb_name,\n",
    "                                                                             estimator.latest_training_job.job_name)\n",
    "profiling_report = sagemaker.Session().read_s3_file(s3_bucket, profiling_report_s3_prefix)\n",
    "\n",
    "\n",
    "# Print debugger's auto-generated profiling report location\n",
    "display(HTML(profiling_report))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-consideration",
   "metadata": {},
   "source": [
    "## 6. Perform custom analysis of the debugger output <a id ='Perform%20custom%20analysis%20of%20the%20debugger%20output'> </a>\n",
    "\n",
    "The debugger's output will be stored in the S3 directories specified in earlier steps.  In this step, we will read that data and display them through various visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-division",
   "metadata": {},
   "source": [
    "### A. Get the training job <a id ='Get%20the%20training%20job'> </a>\n",
    "\n",
    "Get the training job object from the estimator object used for training in the previous step.  This is required to read the metrics from the debugger's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-restaurant",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This assumes that the job was trained in the same AWS region as the S3 bucket where the debugger output is stored\n",
    "# If not, then make appropriate changes to the following code\n",
    "tj = TrainingJob(estimator.latest_training_job.job_name, sagemaker.Session().boto_region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-laugh",
   "metadata": {},
   "source": [
    "### B. Read the metrics <a id ='Read%20the%20metrics'> </a>\n",
    "\n",
    "1. Wait for the the system and framework metrics to be available.\n",
    "2. Get the reader objects for both of these metrics.\n",
    "3. Refresh the event file lists that contains these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-briefing",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Wait for the data to be available\n",
    "tj.wait_for_sys_profiling_data_to_be_available()\n",
    "tj.wait_for_framework_profiling_data_to_be_available()\n",
    "# Get the metrics reader\n",
    "system_metrics_reader = tj.get_systems_metrics_reader()\n",
    "framework_metrics_reader = tj.get_framework_metrics_reader()\n",
    "# Refresh the event file list\n",
    "system_metrics_reader.refresh_event_file_list()\n",
    "framework_metrics_reader.refresh_event_file_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-block",
   "metadata": {},
   "source": [
    "### C. Plot the metrics <a id ='Plot%20the%20metrics'> </a>\n",
    "\n",
    "Plot visualizations for the metrics read in the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-canvas",
   "metadata": {},
   "source": [
    "#### a. System metrics histogram <a id ='System%20metrics%20histogram'> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-routine",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics_histogram = MetricsHistogram(system_metrics_reader)\n",
    "metrics_histogram.plot(\n",
    "    starttime=0, \n",
    "    endtime=system_metrics_reader.get_timestamp_of_latest_available_file(), \n",
    "    select_dimensions=[\"CPU\", \"GPU\", \"I/O\"],\n",
    "    select_events=[\"total\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-uganda",
   "metadata": {},
   "source": [
    "#### b. Framework metrics stepline chart <a id ='Framework%20metrics%20stepline%20chart'> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-welsh",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "view_step_timeline_chart = StepTimelineChart(framework_metrics_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-parallel",
   "metadata": {},
   "source": [
    "#### c. Framework metrics step histogram <a id ='Framework%20metrics%20step%20histogram'> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experimental-saver",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step_histogram = StepHistogram(framework_metrics_reader)\n",
    "step_histogram.plot(\n",
    "    starttime=step_histogram.last_timestamp - 5 * 1000 * 1000, \n",
    "    endtime=step_histogram.last_timestamp, \n",
    "    show_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-obligation",
   "metadata": {},
   "source": [
    "#### d. System and framework metrics timeline charts <a id ='System%20and%20framework%20metrics%20timeline%20charts'> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-david",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_timeline_charts = TimelineCharts(\n",
    "    system_metrics_reader, \n",
    "    framework_metrics_reader,\n",
    "    select_dimensions=[\"CPU\", \"GPU\", \"I/O\"],\n",
    "    select_events=[\"total\"] \n",
    ")\n",
    "\n",
    "view_timeline_charts.plot_detailed_profiler_data([700,710]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-equipment",
   "metadata": {},
   "source": [
    "#### e. System and framework metrics heatmap <a id ='System%20and%20framework%20metrics%20heatmap'> </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-rhythm",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "view_heatmap = Heatmap(\n",
    "    system_metrics_reader,\n",
    "    framework_metrics_reader,\n",
    "    select_dimensions=[\"CPU\", \"GPU\", \"I/O\"],\n",
    "    select_events=[\"total\"],\n",
    "    plot_height=450\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-target",
   "metadata": {},
   "source": [
    "## 7. Cleanup <a id='Cleanup'></a>\n",
    "\n",
    "As a best practice, you should delete resources and S3 objects when no longer required.  This will help you avoid incurring unncessary costs.\n",
    "\n",
    "This step will cleanup the resources and S3 objects created by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-matthew",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Delete data from S3 bucket\n",
    "for file in s3_bucket_resource.objects.filter(Prefix='{}/'.format(nb_name)):\n",
    "    file_key = file.key\n",
    "    print('Deleting {} ...'.format(file_key))\n",
    "    s3_resource.Object(s3_bucket_resource.name, file_key).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-congress",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
